name: Run Node.js Scraper

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: windows-latest

    steps:
    # 1. Check out the repo (full history so we can re-base)
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    # 2. Cache node_modules to avoid reinstalling dependencies
    - name: Cache node_modules
      uses: actions/cache@v3
      with:
        path: |
          node_modules
        key: node-${{ runner.os }}-${{ hashFiles('**/package-lock.json') }}
        restore-keys: |
          node-${{ runner.os }}-

    # 3. Set up Node.js and enable npm-cache
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'package-lock.json'

    # 4. Install dependencies only if they are not cached
    - name: Install dependencies
      run: npm ci

    # 5. Cache Chromium to avoid installation
    - name: Cache Chromium (for Puppeteer)
      uses: actions/cache@v3
      with:
        path: C:\Program Files (x86)\Google\Chrome\Application
        key: chrome-${{ runner.os }}-chromium
        restore-keys: |
          chrome-${{ runner.os }}-

    # 6. Install Chromium if not already cached
    - name: Install Chromium (for Puppeteer)
      run: |
        choco install chromium --no-progress

    # 7. Run the scraper
    - name: Run scraper
      run: |
        # Set the CHROME_BIN environment variable for Puppeteer
        $env:CHROME_BIN="C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
        node scraper.js

    # 8. Cache the scraped data between runs based on the file's content
    - name: Cache scraped data
      uses: actions/cache@v3
      with:
        path: scrapedData.json
        key: scraped-${{ runner.os }}-${{ hashFiles('scrapedData.json') }}
        restore-keys: |
          scraped-${{ runner.os }}-

    # 9. Verify the file exists
    - name: Verify scrapedData.json exists
      shell: pwsh
      run: |
        if (-not (Test-Path "scrapedData.json")) {
          Write-Error "scrapedData.json does not exist!"
        }

    # 10. Commit & push if the file changed
    - name: Commit and push scraped data
      shell: bash
      run: |
        set -e
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

        # Stage the output file
        git add scrapedData.json

        # Exit early if nothing to commit
        if git diff --cached --quiet; then
          echo "No changes to commit."
          exit 0
        fi

        # Commit and re-base on top of any remote changes
        git commit -m "chore: update scraped data"
        git pull --rebase origin main

        # Push our rebased commit
        git push origin HEAD:main
