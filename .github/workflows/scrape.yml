name: Run Node.js Scraper

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: windows-latest

    steps:
    # 1. Check out the repo
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    # 2. Set up Node.js
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    # 3. Pre-check and generate missing dependencies if necessary
    - name: Check for missing dependencies and install if necessary
      run: |
        # Ensure package-lock.json exists and node_modules is installed
        if (-not (Test-Path "package-lock.json")) {
          echo "package-lock.json not found, running npm install to generate it"
          npm install
        }
        
        if (-not (Test-Path "node_modules")) {
          echo "node_modules not found, running npm install"
          npm install
        }

    # 4. Install Chromium (for Puppeteer) using Chocolatey
    - name: Install Chromium (for Puppeteer)
      run: |
        choco install chromium --no-progress -y

    # 5. Run the scraper (Set CHROME_BIN to Chromium installed by Chocolatey)
    - name: Run scraper
      env:
        # Set CHROME_BIN to the path where Chocolatey installs Chromium in GitHub Actions
        CHROME_BIN: "C:\ProgramData\chocolatey\lib\chromium\tools\chrome.exe"
      run: |
        node scraper.js  # Run the scraper script

    # 6. Verify scrapedData.json exists in the correct location in GitHub Actions workspace
    - name: Verify scrapedData.json exists
      shell: pwsh
      run: |
        # Ensure the file exists in the GitHub Actions workspace
        $scrapedDataPath = "${{ github.workspace }}\scrapedData.json"
        if (-not (Test-Path $scrapedDataPath)) {
          Write-Error "scrapedData.json does not exist!"
        } else {
          Write-Host "scrapedData.json exists."
        }

    # 7. Commit and push scraped data if it changed
    - name: Commit and push scraped data
      shell: bash
      run: |
        set -e
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

        git add scrapedData.json

        # Exit early if nothing to commit
        if git diff --cached --quiet; then
          echo "No changes to commit."
          exit 0
        fi

        git commit -m "chore: update scraped data"
        git pull --rebase origin main
        git push origin HEAD:main
