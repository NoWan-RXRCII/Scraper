name: Run Node.js Scraper

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: windows-latest

    steps:
    # 1. Check out the repo
    - uses: actions/checkout@v4

    # 2. Set up Node.js and enable npm-cache support
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'package-lock.json'

    # 3. Install dependencies
    - name: Install dependencies
      run: npm ci

    # 4. Run the scraper
    - name: Run scraper
      run: node scraper.js

    # 5. Cache the scraped JSON between runs
    - name: Cache scraped data
      uses: actions/cache@v3
      with:
        path: scrapedData.json
        key: scraped-${{ runner.os }}-${{ hashFiles('scrapedData.json') }}
        restore-keys: |
          scraped-${{ runner.os }}-

    # 6. Verify the file was created
    - name: Verify scrapedData.json exists
      shell: pwsh
      run: |
        if (-Not (Test-Path "scrapedData.json")) {
          Write-Error "scrapedData.json does not exist!"
        }

    # 7. Commit & push if the file changed
    - name: Commit and push scraped data
      shell: bash
      run: |
        git config --global user.name  "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git add scrapedData.json
        git diff --cached --quiet || git commit -m "Update scraped data"
        git push origin HEAD:main
