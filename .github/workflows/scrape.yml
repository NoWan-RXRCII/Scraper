name: Run Node.js Scraper

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: windows-latest

    steps:
    # 1. Check out the repository
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    # 2. Set up Node.js
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    # 3. Install dependencies
    - name: Install dependencies
      run: npm ci

    # 4. Run the scraper
    - name: Run scraper
      run: |
        # Set the CHROME_BIN environment variable for Puppeteer
        $env:CHROME_BIN="C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
        node scraper.js

    # 5. Commit and push scraped data
    - name: Commit and push scraped data
      shell: bash
      run: |
        set -e
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

        # Stage the output file
        git add scrapedData.json

        # Exit early if nothing to commit
        if git diff --cached --quiet; then
          echo "No changes to commit."
          exit 0
        fi

        # Commit and re-base on top of any remote changes
        git commit -m "chore: update scraped data"
        git pull --rebase origin main

        # Push our rebased commit
        git push origin HEAD:main
