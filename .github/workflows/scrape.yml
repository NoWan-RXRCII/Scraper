name: Run Node.js Scraper

on:
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-scraper:
    runs-on: windows-latest

    steps:
    # 1. Check out the repo (full history so we can re-base)
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    # 2. Set up Node.js and enable npm-cache
    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'package-lock.json'

    # 3. Install dependencies
    - name: Install dependencies
      run: npm ci

    # 4. Run the scraper
    - name: Run scraper
      run: node scraper.js

    # 5. Cache the scraped JSON between runs
    - name: Cache scraped data
      uses: actions/cache@v3
      with:
        path: scrapedData.json
        key: scraped-${{ runner.os }}-${{ hashFiles('scrapedData.json') }}
        restore-keys: scraped-${{ runner.os }}-

    # 6. Verify the file exists
    - name: Verify scrapedData.json exists
      shell: pwsh
      run: |
        if (-not (Test-Path "scrapedData.json")) {
          Write-Error "scrapedData.json does not exist!"
        }

    # 7. Commit & push if the file changed
    - name: Commit and push scraped data
      shell: bash
      run: |
        set -e
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

        # Stage the output file
        git add scrapedData.json

        # Exit early if nothing to commit
        if git diff --cached --quiet; then
          echo "No changes to commit."
          exit 0
        fi

        # Commit and re-base on top of any remote changes
        git commit -m "chore: update scraped data"
        git pull --rebase origin main

        # Push our rebased commit
        git push origin HEAD:main
